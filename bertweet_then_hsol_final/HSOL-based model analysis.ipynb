{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=hsol-finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: simon-andrews (umass-iesl-is). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_PROJECT=hsol-finetuning\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.has_cuda else 'cpu'); device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.has_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = datasets.load_metric(\"accuracy\")\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = datasets.load_metric(\"f1\")\n",
    "\n",
    "def compute_f1(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"f1_micro\": f1_score(labels, predictions, average=\"micro\"),\n",
    "        \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    labels[labels == 2] = 1\n",
    "    predictions[predictions == 2] = 1\n",
    "    cfm = confusion_matrix(labels, predictions)\n",
    "    true_negatives = cfm[0][0]\n",
    "    false_negatives = cfm[1][0]\n",
    "    true_positives = cfm[1][1]\n",
    "    false_positives = cfm[0][1]\n",
    "    return {\n",
    "        \"true_negatives\": true_negatives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(eval_pred):\n",
    "    return {\n",
    "        **compute_accuracy(eval_pred),\n",
    "        **compute_f1(eval_pred),\n",
    "        # **compute_confusion_matrix(eval_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_classes(dataset):\n",
    "    unique_labels = set(dataset[\"label\"])\n",
    "    label_counts = np.ndarray(shape=len(unique_labels), dtype=int)\n",
    "    for label in unique_labels:\n",
    "        label_count = np.count_nonzero(np.array(dataset[\"label\"]) == label)\n",
    "        label_counts[label] = label_count\n",
    "\n",
    "    to_insert = dict()\n",
    "    for label in unique_labels:\n",
    "        to_insert[label] = np.max(label_counts) - label_counts[label]\n",
    "\n",
    "    for label, count in to_insert.items():\n",
    "        matches = dataset.filter(lambda ex: ex['label'] == label)\n",
    "        np.random.seed(685)\n",
    "        idxs = np.random.choice(np.arange(len(matches)), size=count)\n",
    "        dataset = datasets.concatenate_datasets([dataset, matches.select(idxs)])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub('&#\\d+;', '', text) # remove emojis\n",
    "    text = re.sub('RT @.+: ', '', text) # remove retweets\n",
    "    text = re.split(' |\\n', text) # split on whitespace\n",
    "    text = list(map(lambda word: '<username>' if re.match('@.+', word) else word, text))\n",
    "    text = list(map(lambda word: '<url>' if re.match('https?://.+', word) else word, text))\n",
    "    text = list(map(lambda word: '<number>' if re.match('\\d+', word) else word, text))\n",
    "    text = list(map(lambda word: '' if re.match('rt', word) else word, text))\n",
    "    text = ' '.join(text)\n",
    "    text = text.lower() # to lowercase\n",
    "    text = text.translate(text.maketrans('', '', string.punctuation.replace('<', '').replace('>', '')))\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset hate_speech_offensive (/home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-d52ad299245a54d3.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-f54c318528565eb0.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-4ef1853e16f88b2d.arrow\n",
      "Loading cached split indices for dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-eb38c9bfe4615db7.arrow and /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-2fa7a11bef4605f9.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-e9cf6b4c2155b7c5.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-2d9211bb18019e8f.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-fe052913dba68fcb.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-f3163ac71f77174b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 37312\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 --> not hate speech\n",
    "# 1 --> hate speech\n",
    "\n",
    "hsol_data_binary = datasets.load_dataset(\"hate_speech_offensive\", split=\"train\") \\\n",
    "    .rename_column(\"tweet\", \"text\") \\\n",
    "    .rename_column(\"class\", \"label\") \\\n",
    "    .remove_columns([\"count\", \"hate_speech_count\", \"offensive_language_count\", \"neither_count\"]) \\\n",
    "    .map(lambda ex: {\"text\": preprocess_tweet(ex[\"text\"])}) \\\n",
    "    .map(lambda ex: tokenizer(ex[\"text\"], padding=\"max_length\", truncation=True, max_length=64), batched=True) \\\n",
    "    .map(lambda ex: {\"label\": 1 if ex[\"label\"] == 0 else 0}) \\\n",
    "    .train_test_split(test_size=0.2, seed=685)\n",
    "\n",
    "hsol_data_binary[\"train\"] = upsample_classes(hsol_data_binary[\"train\"])\n",
    "hsol_data_binary = hsol_data_binary.shuffle(seed=685)\n",
    "hsol_data_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset hate_speech_offensive (/home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-d52ad299245a54d3.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-f54c318528565eb0.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-bf7e2c55f7d67e09.arrow\n",
      "Loading cached split indices for dataset at /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-5f675205f08f57e5.arrow and /home/ubuntu/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-22af768062265407.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 19826\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 --> not hate speech\n",
    "# 1 --> offensive but not hateful\n",
    "# 2 --> hate speech\n",
    "\n",
    "hsol_data_multiclass = datasets.load_dataset(\"hate_speech_offensive\", split=\"train\") \\\n",
    "    .rename_column(\"tweet\", \"text\") \\\n",
    "    .rename_column(\"class\", \"label\") \\\n",
    "    .remove_columns([\"count\", \"hate_speech_count\", \"offensive_language_count\", \"neither_count\"]) \\\n",
    "    .map(lambda ex: {\"text\": preprocess_tweet(ex[\"text\"])}) \\\n",
    "    .map(lambda ex: tokenizer(ex[\"text\"], padding=\"max_length\", truncation=True, max_length=64), batched=True) \\\n",
    "    .map(lambda ex: {\"label\": 2 if ex[\"label\"] == 0 else 1 if ex[\"label\"] == 1 else 0}) \\\n",
    "    .train_test_split(test_size=0.2, seed=685)\n",
    "\n",
    "# hsol_data_multiclass[\"train\"] = upsample_classes(hsol_data_multiclass[\"train\"])\n",
    "# hsol_data_multiclass = hsol_data_multiclass.shuffle(seed=685)\n",
    "hsol_data_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d30f5d6c38246e2afdb584afd2b297e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21480 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36d05f016a941718d9c1a9e55e8375c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 17184\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4296\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 --> not hate speech\n",
    "# 1 --> hate speech\n",
    "\n",
    "implicit_data_binary = pd.read_table('implicit_hate_v1_stg1_posts.tsv') \\\n",
    "    .rename(columns={\"post\": \"text\", \"class\": \"label\"}) \\\n",
    "    .replace({\"not_hate\": 0, \"implicit_hate\": 1, \"explicit_hate\": 1})\n",
    "\n",
    "implicit_data_binary = datasets.Dataset.from_pandas(implicit_data_binary) \\\n",
    "    .map(lambda ex: {\"text\": preprocess_tweet(ex[\"text\"])}) \\\n",
    "    .map(lambda ex: tokenizer(ex[\"text\"], padding=\"max_length\", truncation=True), batched=True) \\\n",
    "    .remove_columns(\"text\") \\\n",
    "    .train_test_split(test_size=0.2, seed=685)\n",
    "\n",
    "implicit_data_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82c551bca9e45d89fef0b7c18d2ae33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21480 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bee81d0be044bca089c74a056ca14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 17184\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4296\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 --> not hate speech\n",
    "# 1 --> implicit hate speech\n",
    "# 2 --> explicit hate speech\n",
    "\n",
    "implicit_data_multiclass = pd.read_table('implicit_hate_v1_stg1_posts.tsv') \\\n",
    "    .rename(columns={\"post\": \"text\", \"class\": \"label\"}) \\\n",
    "    .replace({\"not_hate\": 0, \"implicit_hate\": 1, \"explicit_hate\": 2})\n",
    "\n",
    "implicit_data_multiclass = datasets.Dataset.from_pandas(implicit_data_multiclass) \\\n",
    "    .map(lambda ex: {\"text\": preprocess_tweet(ex[\"text\"])}) \\\n",
    "    .map(lambda ex: tokenizer(ex[\"text\"], padding=\"max_length\", truncation=True, max_length=64), batched=True) \\\n",
    "    .remove_columns(\"text\") \\\n",
    "    .train_test_split(test_size=0.2, seed=685)\n",
    "\n",
    "implicit_data_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876591dde5a74b2399f7fb253d2ffe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caca3fc9f25e4144995b99940f6c9cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16298\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4093\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 --> not hate speech\n",
    "# 1 --> implicit hate speech\n",
    "implicit_data_non_vs_implicit = implicit_data_multiclass \\\n",
    "    .filter(lambda ex: ex[\"label\"] != 2)\n",
    "\n",
    "implicit_data_non_vs_implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77e610e9c2d4a70905d91947e8cb149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6346 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 6346\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 --> incitement\n",
    "# 1 --> inferiority\n",
    "# 2 --> irony\n",
    "# 3 --> stereotypical\n",
    "# 4 --> threatening\n",
    "# 5 --> white_grievance\n",
    "# 6 --> other\n",
    "\n",
    "implicit_data_fine = pd.read_table('implicit_hate_v1_stg2_posts.tsv') \\\n",
    "    .rename(columns={\"post\": \"text\", \"implicit_class\": \"label\"}) \\\n",
    "    .drop(\"extra_implicit_class\", axis=1) \\\n",
    "    .replace({\"incitement\": 0, \"inferiority\": 1, \"irony\": 2, \"stereotypical\": 3, \"threatening\": 4, \"white_grievance\": 5, \"other\": 6})\n",
    "\n",
    "implicit_data_fine = datasets.Dataset.from_pandas(implicit_data_fine) \\\n",
    "    .map(lambda ex: {\"text\": preprocess_tweet(ex[\"text\"])})\n",
    "\n",
    "implicit_data_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jewish harvard professor noel ignatiev wants to abolish the white race via wr',\n",
       " 'tune in todays jmt for my interview w  robert spencer on   the complete infidels guide to iran',\n",
       " 'hate posters  you want hate buddy  read the koran  goo',\n",
       " 'hunt for white women  black racist serial killer confesses to <number> murders',\n",
       " 'looks like he was a fan of the islamic jewhate reza aslan peddles as critique of i',\n",
       " 'antiwhites hate white solidarity  their hate for white european self preservation knows no',\n",
       " 'are forever white racemass murderers',\n",
       " 'i think white people are waking up in unprecedented numbers',\n",
       " 'im not a racist at all but i mean all the numbers are saying caucaisian people are set to disappear soon so im for keeping the race',\n",
       " 'its a planned growth of africans  a planned elimination of whites  whitegenocide  kalergi  maga tcot trump altright sjw buildawall',\n",
       " 'its ok to be white  the book',\n",
       " 'now it is full of jew and government infiltrators',\n",
       " 'only racist if youre white  this just makes you smart',\n",
       " 'prof kmac is right all along that jews have been manipulate whiteeuro people',\n",
       " 'they say they are antiracist  but what they really are is antiwhite  via',\n",
       " 'latest unhinged antiwhite rant at ferguson   ugly black lesbian  negrogeddon',\n",
       " 'new zealand  white faces will soon be a minority via',\n",
       " 'optics frat    i  e     im interested in being a spectacle for the antiwhite media  not organizing white people into a cohesive prowhite movement    being a clown will get your media coverage  is your goal is to get on tv for a <number> minute hate',\n",
       " 'we must have an allwhite america  in which our children and our grandchildren will play and go to school with other white children     glr',\n",
       " 'what our bros  sis experiencing in me  what is coming for us in west  must rise up and awaken    bgun',\n",
       " 'african   refugee   was only here nine days before raping a woman  full story',\n",
       " 'and of course    antiracist   is a code word for   antiwhite      selfhating jews   are nothing compared with selfhating white people',\n",
       " 'i like jared taylor    he white',\n",
       " 'race cucks like paul ryan should be banished to live in a   diverse   neighbrhd if they want t spout such nonsense',\n",
       " 'right  what good is   white supremacy   if it leads to a white minority',\n",
       " 'proud white women speak out against white genocide  video  via',\n",
       " 'i hate you all    racist note from the black boy who raped his white teacher  slit her throat via wr',\n",
       " 'how jews feel about whitepeople    the white race is the cancer of human history     jewess susan sontag hate israel  r',\n",
       " 'white racialists rally in edmonton  alberta and london  ontario as a commemoration of   white pride worldwide day',\n",
       " 'another hate hoax  police nab notorious   white supremacist   who drew swastikas around a college',\n",
       " 'australia returns nonwhite invaders  jewish   human rights   groups flip out  more',\n",
       " 'black people cant be racist    ohio man pushes white jogger into traffic because he   hates whites   via',\n",
       " 'dawa  > barbaric  totalitarian political ideology intent on enslaving all human beings for   allah   imagine founding a club on these principles',\n",
       " 'since when was jesus   mixed race     please stop trying to stick jesus and christianity into your criminal communist ideology  dont tell me youre like those bbc antiwhite racists that depict julius caesar and achilles as black men in the name of   diversity   and inclusiveness',\n",
       " 'they smother the good white people of america  they need to be hung  brother   south',\n",
       " 'uk sport displays anti white racism when they cut funding for   white sports   prior to tokyo olympics',\n",
       " 'us should have leaders this brave     i dont want blacks in estonia  this is a white country   via',\n",
       " 'altrightmeans becoming great again',\n",
       " 'aryan men  women will either rise up to fight for their heritage  children   future  or the white race will cease to exist  choose now',\n",
       " 'blacklivesmatter and again  black on white crime  what a surprise',\n",
       " 'defundnfl stop paying to make people who hate you  your nation  your people rich  wakeupwhiteman',\n",
       " 'iffootballdidntexist white men would fulfill their tribal instinct by standing up for their own kind  not worshipping negros who hate them',\n",
       " 'kkk  i do not mind your booklet  how can i support',\n",
       " 'ukraine ukrainian thanks brothers  white pride worldwide  wpww 14words',\n",
       " 'hey watch this  leftism  enabling islamists since times before crusades  historyrepeats',\n",
       " 'op ed  islamic doctrine of shariah is anticonstitutional  your future potus should agree',\n",
       " 'and throw in weekly standard these filthy white trash disgust me',\n",
       " 'csp poll  <number>  <number>  of muslimamericans  violence against those who insult muhammad is acceptable',\n",
       " 'elin krantz  just another dead white liberal female raped to death by nonwhite invader',\n",
       " 'hollywood jew harvey weinstein busted for decadeslong sexual abuse of white women via',\n",
       " 'jewish harvard professor wants to abolish the white race  read more  via',\n",
       " 'kanye west releases his most antiwhite rap song ever sucks hes the most racist man alive',\n",
       " 'nice to know that tsunami didnt dampen their energy to stay in the <number> century',\n",
       " 'uk  students in islamic schools taught sharia amputation methods and that jews are conspiring to take over',\n",
       " 'where is the media outrage  white man was lynched and robbed by a racist black gang   video  via',\n",
       " 'whores of babylon  lena dunham celebrates white genocide via whitegenocide waronwhites',\n",
       " 'a certain <number> years old religious ideology has proven to be the biggest killing machine of innocent people in human history',\n",
       " 'a future for white children  in the same fashon as hitler youth  who were given the great honor of dying for hitler  seig heil  amen',\n",
       " 'all jews should be evicted from white countries',\n",
       " 'all white are not aryans  why are they trying to board the bandwagon of aryans',\n",
       " 'angola used to be a prosperous portuguese colony  whites fled en masse in <number>',\n",
       " 'another white fourteen year old gangraped by africans in norway via wpww',\n",
       " 'antiwhite tweets from hijab rio2016 athlete  deportallmuslims',\n",
       " 'are we actually fighting an asymmetrical war against white genocide or are we just playing tiddlywinks with enemies who mean to kill us',\n",
       " 'are you jewish  if so you arent white  either way youre antiwhite',\n",
       " 'basic human rightswhat for so how are u guys on jews and blacksben n i hate them',\n",
       " 'because of course whitepeople need billboard marketing kkk kkk',\n",
       " 'before i read it was white pride i was going to suggest white only sessions',\n",
       " 'belgium  nonwhite invader rapes white woman after dont rape course  via',\n",
       " 'both north america and europe belong to us  white men conquered and colonized both',\n",
       " 'britain sees race war brewing between nonwhite invaders  read more',\n",
       " 'can whites have black artists shoved down their throats without losing their own culture and heritage',\n",
       " 'come together white people or perish',\n",
       " 'dont tell bernie  hescandinavian socialism  will shudder islam invasion',\n",
       " 'dont think the white public wont turn against the blm unless the lying media tells them to  the white public rejects the media',\n",
       " 'dont worry  bill  its not a pogrom  were just replacing jews with white people',\n",
       " 'elin krantz  just another dead white liberal female raped to death by nonwhite invader via',\n",
       " 'extinction of americas white population speeds up swrm whitegenocide prowhite speedsup via',\n",
       " 'final solution  the <number> plan to exterminate germans whitegenocide munich prayformunich',\n",
       " 'german holocaust revisionist patriot horst mahler arrested in hungary via',\n",
       " 'hate group because your proud to be a white american christian  insanity  and very disappointing',\n",
       " 'he is also a black racist  dont forget that',\n",
       " 'help us in california  they are killing americans jobs and taking over the state  some wont go out if they dont have to  a lot dont speak english  they have taken over everything  the politicians are not for americans  they are for illegals  cant move',\n",
       " 'heretics  infidels halal to kill',\n",
       " 'horrible  <number> year old white boy commits suicide after being tormented for months by racist arabs via',\n",
       " 'i dont think his message of racial inclusion was aimed at minorities  i think it was aimed at cucky whites',\n",
       " 'i stick up for white people  thats not what youre are racist and putting down other races',\n",
       " 'i support the alt right aspersions for portion of us to be all white ethnostate  even',\n",
       " 'i support the white brotherhood',\n",
       " 'i take that back  one of his parents must be white  not purebred',\n",
       " 'i was born white  i cant help it  i should just kill myself  said no one ever',\n",
       " 'if a muslim kill a jew they celebrate him as a hero  if a christian kill a jew we call',\n",
       " 'if its white its right',\n",
       " 'interracial marriage  white genocide',\n",
       " 'is white nationalism an adl honey pot front',\n",
       " 'islam is racist',\n",
       " 'isnt that how it usually begins  this hatred for infidels and belief in islamic supremacy etc',\n",
       " 'it would be nice  i would like the idea of my white race surviving  to thrive and kick a',\n",
       " 'its fundamentally different  whites are better off without blacks and jews  both parasites on white civilization',\n",
       " 'its us white gentiles getting screwed']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit_data_fine[0:100][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model on binary HSOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 37312\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1749\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/notebooks/NLP project/wandb/run-20220511_164210-3vxa5xwq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/umass-iesl-is/hsol-finetuning/runs/3vxa5xwq\" target=\"_blank\">bert-then-hsol-binary</a></strong> to <a href=\"https://wandb.ai/umass-iesl-is/hsol-finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1749/1749 31:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.513783</td>\n",
       "      <td>0.755699</td>\n",
       "      <td>0.755699</td>\n",
       "      <td>0.822696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.607534</td>\n",
       "      <td>0.755901</td>\n",
       "      <td>0.755901</td>\n",
       "      <td>0.822956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.361355</td>\n",
       "      <td>0.850918</td>\n",
       "      <td>0.850918</td>\n",
       "      <td>0.886252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.303698</td>\n",
       "      <td>0.886020</td>\n",
       "      <td>0.886020</td>\n",
       "      <td>0.908681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.304908</td>\n",
       "      <td>0.886020</td>\n",
       "      <td>0.886020</td>\n",
       "      <td>0.909211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.258688</td>\n",
       "      <td>0.915877</td>\n",
       "      <td>0.915877</td>\n",
       "      <td>0.927686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>0.227159</td>\n",
       "      <td>0.926165</td>\n",
       "      <td>0.926165</td>\n",
       "      <td>0.934434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.228077</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>0.933568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.124800</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.934840</td>\n",
       "      <td>0.934840</td>\n",
       "      <td>0.939120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.221379</td>\n",
       "      <td>0.934234</td>\n",
       "      <td>0.934234</td>\n",
       "      <td>0.939507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.077200</td>\n",
       "      <td>0.275228</td>\n",
       "      <td>0.923139</td>\n",
       "      <td>0.923139</td>\n",
       "      <td>0.931861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.337821</td>\n",
       "      <td>0.918096</td>\n",
       "      <td>0.918096</td>\n",
       "      <td>0.928869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.941295</td>\n",
       "      <td>0.941295</td>\n",
       "      <td>0.942659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.277020</td>\n",
       "      <td>0.936857</td>\n",
       "      <td>0.936857</td>\n",
       "      <td>0.938928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.282193</td>\n",
       "      <td>0.938269</td>\n",
       "      <td>0.938269</td>\n",
       "      <td>0.937461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.320391</td>\n",
       "      <td>0.935848</td>\n",
       "      <td>0.935848</td>\n",
       "      <td>0.938002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.305831</td>\n",
       "      <td>0.932621</td>\n",
       "      <td>0.932621</td>\n",
       "      <td>0.935694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.332363</td>\n",
       "      <td>0.933226</td>\n",
       "      <td>0.933226</td>\n",
       "      <td>0.935825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.346123</td>\n",
       "      <td>0.942707</td>\n",
       "      <td>0.942707</td>\n",
       "      <td>0.940442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.343939</td>\n",
       "      <td>0.937059</td>\n",
       "      <td>0.937059</td>\n",
       "      <td>0.936944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.357342</td>\n",
       "      <td>0.936454</td>\n",
       "      <td>0.936454</td>\n",
       "      <td>0.937184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.374848</td>\n",
       "      <td>0.937664</td>\n",
       "      <td>0.937664</td>\n",
       "      <td>0.939010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.378208</td>\n",
       "      <td>0.932419</td>\n",
       "      <td>0.932419</td>\n",
       "      <td>0.936679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.350148</td>\n",
       "      <td>0.936454</td>\n",
       "      <td>0.936454</td>\n",
       "      <td>0.938238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.358181</td>\n",
       "      <td>0.937866</td>\n",
       "      <td>0.937866</td>\n",
       "      <td>0.938527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.380891</td>\n",
       "      <td>0.942102</td>\n",
       "      <td>0.942102</td>\n",
       "      <td>0.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.346873</td>\n",
       "      <td>0.942506</td>\n",
       "      <td>0.942506</td>\n",
       "      <td>0.939535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.378208</td>\n",
       "      <td>0.942304</td>\n",
       "      <td>0.942304</td>\n",
       "      <td>0.939126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.382134</td>\n",
       "      <td>0.941699</td>\n",
       "      <td>0.941699</td>\n",
       "      <td>0.939456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.367976</td>\n",
       "      <td>0.943111</td>\n",
       "      <td>0.943111</td>\n",
       "      <td>0.940235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.394991</td>\n",
       "      <td>0.939076</td>\n",
       "      <td>0.939076</td>\n",
       "      <td>0.938511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.386199</td>\n",
       "      <td>0.941295</td>\n",
       "      <td>0.941295</td>\n",
       "      <td>0.938525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.390351</td>\n",
       "      <td>0.940488</td>\n",
       "      <td>0.940488</td>\n",
       "      <td>0.938451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.382999</td>\n",
       "      <td>0.942102</td>\n",
       "      <td>0.942102</td>\n",
       "      <td>0.938847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-then-hsol-binary/checkpoint-500\n",
      "Configuration saved in bert-then-hsol-binary/checkpoint-500/config.json\n",
      "Model weights saved in bert-then-hsol-binary/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-then-hsol-binary/checkpoint-1000\n",
      "Configuration saved in bert-then-hsol-binary/checkpoint-1000/config.json\n",
      "Model weights saved in bert-then-hsol-binary/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-then-hsol-binary/checkpoint-1500\n",
      "Configuration saved in bert-then-hsol-binary/checkpoint-1500/config.json\n",
      "Model weights saved in bert-then-hsol-binary/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in bert-then-hsol-binary/config.json\n",
      "Model weights saved in bert-then-hsol-binary/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-cased\").to(device)\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"bert-then-hsol-binary\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"bert-then-hsol-binary\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=64,\n",
    "    eval_steps=50,\n",
    "    logging_steps=20,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hsol_data_binary[\"train\"],\n",
    "    eval_dataset=hsol_data_binary[\"test\"],\n",
    "    compute_metrics=compute_all_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"bert-then-hsol-binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19826\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 930\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/notebooks/NLP project/wandb/run-20220511_183514-3oe889tf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/umass-iesl-is/hsol-finetuning/runs/3oe889tf\" target=\"_blank\">bert-then-hsol-multiclass</a></strong> to <a href=\"https://wandb.ai/umass-iesl-is/hsol-finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [930/930 14:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.322900</td>\n",
       "      <td>0.252069</td>\n",
       "      <td>0.912245</td>\n",
       "      <td>0.912245</td>\n",
       "      <td>0.888233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.279500</td>\n",
       "      <td>0.243769</td>\n",
       "      <td>0.913859</td>\n",
       "      <td>0.913859</td>\n",
       "      <td>0.905973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.238251</td>\n",
       "      <td>0.916683</td>\n",
       "      <td>0.916683</td>\n",
       "      <td>0.916878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.238338</td>\n",
       "      <td>0.912245</td>\n",
       "      <td>0.912245</td>\n",
       "      <td>0.902585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>0.230743</td>\n",
       "      <td>0.915070</td>\n",
       "      <td>0.915070</td>\n",
       "      <td>0.914042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.232540</td>\n",
       "      <td>0.920920</td>\n",
       "      <td>0.920920</td>\n",
       "      <td>0.917987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.156700</td>\n",
       "      <td>0.248942</td>\n",
       "      <td>0.917894</td>\n",
       "      <td>0.917894</td>\n",
       "      <td>0.915890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.260986</td>\n",
       "      <td>0.916885</td>\n",
       "      <td>0.916885</td>\n",
       "      <td>0.915334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>0.254654</td>\n",
       "      <td>0.915877</td>\n",
       "      <td>0.915877</td>\n",
       "      <td>0.914176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-then-hsol-multiclass/checkpoint-500\n",
      "Configuration saved in bert-then-hsol-multiclass/checkpoint-500/config.json\n",
      "Model weights saved in bert-then-hsol-multiclass/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4957\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in bert-then-hsol-multiclass/config.json\n",
      "Model weights saved in bert-then-hsol-multiclass/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3).to(device)\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"bert-then-hsol-multiclass\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"bert-then-hsol-multiclass\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=20,\n",
    "    per_device_train_batch_size=64,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hsol_data_multiclass[\"train\"],\n",
    "    eval_dataset=hsol_data_multiclass[\"test\"],\n",
    "    compute_metrics=compute_all_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"bert-then-hsol-multiclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./bert-then-hsol-multiclass/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./bert-then-hsol-multiclass/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./bert-then-hsol-multiclass.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 17184\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 807\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/notebooks/NLP project/wandb/run-20220511_190710-1f0p0uhg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/umass-iesl-is/hsol-finetuning/runs/1f0p0uhg\" target=\"_blank\">hsol-multiclass-then-implicit-multiclass</a></strong> to <a href=\"https://wandb.ai/umass-iesl-is/hsol-finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 14:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>0.657388</td>\n",
       "      <td>0.689711</td>\n",
       "      <td>0.689711</td>\n",
       "      <td>0.669289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.632360</td>\n",
       "      <td>0.707402</td>\n",
       "      <td>0.707402</td>\n",
       "      <td>0.680949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.633476</td>\n",
       "      <td>0.713222</td>\n",
       "      <td>0.713222</td>\n",
       "      <td>0.702077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.618286</td>\n",
       "      <td>0.717412</td>\n",
       "      <td>0.717412</td>\n",
       "      <td>0.714428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.590512</td>\n",
       "      <td>0.733706</td>\n",
       "      <td>0.733706</td>\n",
       "      <td>0.725132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.609772</td>\n",
       "      <td>0.728585</td>\n",
       "      <td>0.728585</td>\n",
       "      <td>0.710317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.492200</td>\n",
       "      <td>0.641132</td>\n",
       "      <td>0.728119</td>\n",
       "      <td>0.728119</td>\n",
       "      <td>0.725921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.511100</td>\n",
       "      <td>0.598364</td>\n",
       "      <td>0.731844</td>\n",
       "      <td>0.731844</td>\n",
       "      <td>0.725463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.480900</td>\n",
       "      <td>0.621230</td>\n",
       "      <td>0.736266</td>\n",
       "      <td>0.736266</td>\n",
       "      <td>0.721599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.613282</td>\n",
       "      <td>0.737197</td>\n",
       "      <td>0.737197</td>\n",
       "      <td>0.725455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.439400</td>\n",
       "      <td>0.670456</td>\n",
       "      <td>0.739525</td>\n",
       "      <td>0.739525</td>\n",
       "      <td>0.730133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>0.720579</td>\n",
       "      <td>0.734870</td>\n",
       "      <td>0.734870</td>\n",
       "      <td>0.728310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.700840</td>\n",
       "      <td>0.729749</td>\n",
       "      <td>0.729749</td>\n",
       "      <td>0.727941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.319600</td>\n",
       "      <td>0.720429</td>\n",
       "      <td>0.733939</td>\n",
       "      <td>0.733939</td>\n",
       "      <td>0.731059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.731145</td>\n",
       "      <td>0.731145</td>\n",
       "      <td>0.729671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.296800</td>\n",
       "      <td>0.712292</td>\n",
       "      <td>0.728818</td>\n",
       "      <td>0.728818</td>\n",
       "      <td>0.727180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to hsol-multiclass-then-implicit-multiclass/checkpoint-500\n",
      "Configuration saved in hsol-multiclass-then-implicit-multiclass/checkpoint-500/config.json\n",
      "Model weights saved in hsol-multiclass-then-implicit-multiclass/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4296\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in hsol-multiclass-then-implicit-multiclass/config.json\n",
      "Model weights saved in hsol-multiclass-then-implicit-multiclass/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(\"./bert-then-hsol-multiclass\", num_labels=3).to(device)\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"hsol-multiclass-then-implicit-multiclass\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"hsol-multiclass-then-implicit-multiclass\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    per_device_train_batch_size=64,\n",
    "    logging_steps=20,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=implicit_data_multiclass[\"train\"],\n",
    "    eval_dataset=implicit_data_multiclass[\"test\"],\n",
    "    compute_metrics=compute_all_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"hsol-multiclass-then-implicit-multiclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16298\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 765\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='765' max='765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [765/765 11:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.504652</td>\n",
       "      <td>0.742976</td>\n",
       "      <td>0.742976</td>\n",
       "      <td>0.731466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.488934</td>\n",
       "      <td>0.753237</td>\n",
       "      <td>0.753237</td>\n",
       "      <td>0.743944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.394800</td>\n",
       "      <td>0.494043</td>\n",
       "      <td>0.760567</td>\n",
       "      <td>0.760567</td>\n",
       "      <td>0.756225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.505437</td>\n",
       "      <td>0.763987</td>\n",
       "      <td>0.763987</td>\n",
       "      <td>0.755989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.490211</td>\n",
       "      <td>0.769607</td>\n",
       "      <td>0.769607</td>\n",
       "      <td>0.767726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.605169</td>\n",
       "      <td>0.760078</td>\n",
       "      <td>0.760078</td>\n",
       "      <td>0.757790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.614214</td>\n",
       "      <td>0.763743</td>\n",
       "      <td>0.763743</td>\n",
       "      <td>0.763401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to hsol-binary-then-implicit-hate-vs-non/checkpoint-500\n",
      "Configuration saved in hsol-binary-then-implicit-hate-vs-non/checkpoint-500/config.json\n",
      "Model weights saved in hsol-binary-then-implicit-hate-vs-non/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in hsol-binary-then-implicit-hate-vs-non/config.json\n",
      "Model weights saved in hsol-binary-then-implicit-hate-vs-non/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(\"./bert-then-hsol-binary\", num_labels=2).to(device)\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"hsol-binary-then-implicit-hate-vs-non\",\n",
    "    report_to=\"none\",\n",
    "    run_name=\"hsol-binary-then-implicit-hate-vs-non\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=20,\n",
    "    per_device_train_batch_size=64,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=implicit_data_non_vs_implicit[\"train\"],\n",
    "    eval_dataset=implicit_data_non_vs_implicit[\"test\"],\n",
    "    compute_metrics=compute_all_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"hsol-binary-then-implicit-hate-vs-non\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22984abcc5624bcd9563ebe7b29dfcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32332 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41af5c593bbb4912b93968aeda179903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25865\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 6467\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarc_data = pd.read_csv('SARC2/sarc_processed.csv') \\\n",
    "    .drop(columns=['Unnamed: 0']) \\\n",
    "    .sample(frac=0.5) \\\n",
    "    .dropna()\n",
    "\n",
    "sarc_data = datasets.Dataset.from_pandas(sarc_data) \\\n",
    "    .map(lambda ex: {\"text\": preprocess_tweet(ex[\"text\"])}) \\\n",
    "    .map(lambda ex: tokenizer(ex[\"text\"], padding=\"max_length\", truncation=True, max_length=64), batched=True) \\\n",
    "    .remove_columns(\"text\") \\\n",
    "    .train_test_split(test_size=0.2, seed=685) \\\n",
    "    .remove_columns(['__index_level_0__'])\n",
    "\n",
    "sarc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25865\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1215\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: simon-andrews (umass-iesl-is). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/notebooks/NLP project/wandb/run-20220512_024944-1747o718</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/umass-iesl-is/huggingface/runs/1747o718\" target=\"_blank\">bert-then-sarc</a></strong> to <a href=\"https://wandb.ai/umass-iesl-is/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1215' max='1215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1215/1215 20:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.677700</td>\n",
       "      <td>0.661592</td>\n",
       "      <td>0.597340</td>\n",
       "      <td>0.597340</td>\n",
       "      <td>0.586072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.668900</td>\n",
       "      <td>0.634709</td>\n",
       "      <td>0.644348</td>\n",
       "      <td>0.644348</td>\n",
       "      <td>0.642282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.633800</td>\n",
       "      <td>0.618442</td>\n",
       "      <td>0.659038</td>\n",
       "      <td>0.659038</td>\n",
       "      <td>0.650149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.617400</td>\n",
       "      <td>0.604760</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.671645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>0.612572</td>\n",
       "      <td>0.671563</td>\n",
       "      <td>0.671563</td>\n",
       "      <td>0.671545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.562900</td>\n",
       "      <td>0.635252</td>\n",
       "      <td>0.661358</td>\n",
       "      <td>0.661358</td>\n",
       "      <td>0.660389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.533300</td>\n",
       "      <td>0.642238</td>\n",
       "      <td>0.669398</td>\n",
       "      <td>0.669398</td>\n",
       "      <td>0.667883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.622533</td>\n",
       "      <td>0.678831</td>\n",
       "      <td>0.678831</td>\n",
       "      <td>0.678097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.718209</td>\n",
       "      <td>0.665997</td>\n",
       "      <td>0.665997</td>\n",
       "      <td>0.665627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.737332</td>\n",
       "      <td>0.667698</td>\n",
       "      <td>0.667698</td>\n",
       "      <td>0.667396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.346500</td>\n",
       "      <td>0.748169</td>\n",
       "      <td>0.667543</td>\n",
       "      <td>0.667543</td>\n",
       "      <td>0.667084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.750473</td>\n",
       "      <td>0.670945</td>\n",
       "      <td>0.670945</td>\n",
       "      <td>0.670925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-then-sarc/checkpoint-500\n",
      "Configuration saved in bert-then-sarc/checkpoint-500/config.json\n",
      "Model weights saved in bert-then-sarc/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-then-sarc/checkpoint-1000\n",
      "Configuration saved in bert-then-sarc/checkpoint-1000/config.json\n",
      "Model weights saved in bert-then-sarc/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6467\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in bert-then-sarc/config.json\n",
      "Model weights saved in bert-then-sarc/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2).to(device)\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"bert-then-sarc\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"bert-then-sarc\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=20,\n",
    "    per_device_train_batch_size=64,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=sarc_data[\"train\"],\n",
    "    eval_dataset=sarc_data[\"test\"],\n",
    "    compute_metrics=compute_all_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"bert-then-sarc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181370fed4104aa6bbefd679ab2dac29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16669397"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarc_data[\"train\"].to_csv(\"SARC2/train-final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c087837510e43a982fb7e89a3dd47c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4164649"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarc_data[\"test\"].to_csv(\"SARC2/test-final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 25865\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarc_data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16298\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 765\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/notebooks/NLP project/wandb/run-20220512_162946-35g0zowz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/umass-iesl-is/hsol-finetuning/runs/35g0zowz\" target=\"_blank\">sarc-then-implicit-hate-vs-non</a></strong> to <a href=\"https://wandb.ai/umass-iesl-is/hsol-finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='765' max='765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [765/765 11:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.509153</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.728623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.484106</td>\n",
       "      <td>0.758857</td>\n",
       "      <td>0.758857</td>\n",
       "      <td>0.753051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.414100</td>\n",
       "      <td>0.496970</td>\n",
       "      <td>0.757391</td>\n",
       "      <td>0.757391</td>\n",
       "      <td>0.750679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.502297</td>\n",
       "      <td>0.766430</td>\n",
       "      <td>0.766430</td>\n",
       "      <td>0.760560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.389600</td>\n",
       "      <td>0.501058</td>\n",
       "      <td>0.766186</td>\n",
       "      <td>0.766186</td>\n",
       "      <td>0.761148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.620560</td>\n",
       "      <td>0.758368</td>\n",
       "      <td>0.758368</td>\n",
       "      <td>0.755906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.634312</td>\n",
       "      <td>0.759101</td>\n",
       "      <td>0.759101</td>\n",
       "      <td>0.759029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to sarc-then-implicit-hate-vs-non/checkpoint-500\n",
      "Configuration saved in sarc-then-implicit-hate-vs-non/checkpoint-500/config.json\n",
      "Model weights saved in sarc-then-implicit-hate-vs-non/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4093\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in sarc-then-implicit-hate-vs-non/config.json\n",
      "Model weights saved in sarc-then-implicit-hate-vs-non/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(\"./bert-then-sarc\").to(device)\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"sarc-then-implicit-hate-vs-non\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"sarc-then-implicit-hate-vs-non\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=20,\n",
    "    per_device_train_batch_size=64,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=implicit_data_non_vs_implicit[\"train\"],\n",
    "    eval_dataset=implicit_data_non_vs_implicit[\"test\"],\n",
    "    compute_metrics=compute_all_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"sarc-then-implicit-hate-vs-non\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
